{
 "cells": [
  {
   "cell_type": "raw",
   "id": "92ff5289-6b09-4678-a161-1c41144ce57e",
   "metadata": {},
   "source": [
    "Q1) R2 is a statistical value which shows the proportion of variation in the dependent variable that is explained the \n",
    "    independent variable of a regression model. \n",
    "    It is given by R2 = 1 - (SS_residuals/SS_total)."
   ]
  },
  {
   "cell_type": "raw",
   "id": "aa9bb39c-1fd6-4785-8b14-c2bcedcbd05e",
   "metadata": {},
   "source": [
    "Q2) Adjusted R2 takes into accounts the indepedent features used, and is more accurate when multiple independent \n",
    "    features is used."
   ]
  },
  {
   "cell_type": "raw",
   "id": "02bf7393-21f8-4c2d-a3be-ed00dfc8aabf",
   "metadata": {},
   "source": [
    "Q3) When multiple independent features are used, we use adjusted R2."
   ]
  },
  {
   "cell_type": "raw",
   "id": "f09fbf10-8812-4dee-8f8e-20833ae84574",
   "metadata": {},
   "source": [
    "Q4) These are cost functions. MSE: mean_squared_error , MAE: mean_absolute_error , RMSE=root_mean_squared_error"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f9685cc3-a029-4a77-a235-6b91cb8c5bec",
   "metadata": {},
   "source": [
    "Q5)MSE: is not robust to outliers, not of same unit as target\n",
    "   MAE: robust to outliers but convergence takes timea and optimisation is an issue"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e4ebd97d-4fe2-4a3b-9558-2082f106bdc3",
   "metadata": {},
   "source": [
    "Q6) Lasso Regularisation is a L1 regularisation in which the penalty term contains linear summation of slopes.\n",
    "    Ridge Regularisation is a L2 regularisation in which the penalty term contain  summation of squares of slopes.\n",
    "    We use Ridge Regression to avoid overfitting and Lasso for feature selection."
   ]
  },
  {
   "cell_type": "raw",
   "id": "35aed44b-4bc1-467c-a617-630633f9e333",
   "metadata": {},
   "source": [
    "Q7) When a regression model overfits the accuracy on train data is high but on test data is low, the cost function\n",
    "    becomes 0 for overfitting and no further covergence(gradient descent) is possible, so we apply a penalty term\n",
    "    to the cost function to avoid such issue."
   ]
  },
  {
   "cell_type": "raw",
   "id": "a0527d6c-840a-4152-911a-82fe8a4a9b5e",
   "metadata": {},
   "source": [
    "Q9) Model A with RMSE=10 is better performaing than Model B with MAE=8 , beacause RMSE penalises heavily if errors are large\n",
    "    because it is calculated on square of the difference whereas MAE is only linear geting a low RMSE score is tougher \n",
    "    than getting a low MAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dcd12c-b2dd-4f7c-92d2-e48dbf276c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q10) Model with Ridge Regularisation will be better performer "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
